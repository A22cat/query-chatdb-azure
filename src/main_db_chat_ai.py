# ãƒ¡ã‚¤ãƒ³ç”»é¢
import streamlit as st
import pyodbc
import pandas as pd
import openai
import os
import json
import re
import datetime
import uuid

from dotenv import load_dotenv

from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.models import VectorizedQuery
from azure.search.documents.indexes.models import (
    SearchIndex,
    SearchField,
    SearchFieldDataType,
    SimpleField,
    SearchableField,
    VectorSearch,
    HnswAlgorithmConfiguration, # HNSWã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ç”¨
    VectorSearchProfile
#    SemanticConfiguration, # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ç”¨ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
#    SemanticPrioritizedFields,
#    SemanticField
)
import tiktoken # ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

# ãƒšãƒ¼ã‚¸ã®åŸºæœ¬è¨­å®š
st.set_page_config(
    page_title="ã‚¯ã‚¨ãƒªãƒãƒ£ãƒƒãƒˆDB Azure(Qnect)(ã‚­ãƒ¥ãƒã‚¯ãƒˆ)",
    page_icon="ğŸ”—",
    layout="wide",
)

# ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜
st.title("ğŸ”— Qnect (ã‚­ãƒ¥ãƒã‚¯ãƒˆ)")
st.caption("Azure OpenAIã¨Azure AI Searchã‚’æ´»ç”¨ã—ãŸå¯¾è©±å‹DBã‚¯ã‚¨ãƒªæ”¯æ´ã‚·ã‚¹ãƒ†ãƒ ")

# .envã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# Azure OpenAI ã®è¨­å®š
client = openai.AzureOpenAI(
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
)

# Azure AI Search ã®è¨­å®š
AZURE_SEARCH_ENDPOINT = os.getenv("AZURE_SEARCH_ENDPOINT")
AZURE_SEARCH_KEY = os.getenv("AZURE_SEARCH_KEY")
AZURE_SEARCH_INDEX_NAME = os.getenv("AZURE_SEARCH_INDEX_NAME", "chat-history-index")
AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME")


# DBæ¥ç¶šæ–‡å­—åˆ—ã®å–å¾—
from db_config import connection_string

# ãƒã‚¹ã‚­ãƒ³ã‚°è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
#with open("config/data_masking/masking_columns.json", "r", encoding="utf-8") as f:
#    masking_config = json.load(f)
masking_config = {}
masking_path = "config/data_masking/masking_columns.json"
if os.path.exists(masking_path):
    with open(masking_path, "r", encoding="utf-8") as f:
        masking_config = json.load(f)

def get_mask_columns(table_name):
    return masking_config.get(table_name, [])

# å±¥æ­´ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«
HISTORY_PATH = "data/chat_history.json"

# --- ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ¶é™ã®ãŸã‚ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
def trim_messages_to_token_limit(messages, max_tokens=10000, model_name="gpt-4o-mini"):
    """æŒ‡å®šã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³æ•°ä¸Šé™ã‚’è¶…ãˆãªã„ã‚ˆã†ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°ã™ã‚‹"""
    try:
        enc = tiktoken.encoding_for_model(model_name)
    except KeyError:
        # ãƒ¢ãƒ‡ãƒ«åãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨ (ä¾‹: gpt-4)
        enc = tiktoken.get_encoding("cl100k_base")

    total_tokens = 0
    trimmed_messages = []
    # messages ã¯ [ {"role": "system", "content": "..."}, {"role": "user", "content": "..."} ] ã®å½¢å¼ã‚’æƒ³å®š
    # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å„ªå…ˆã™ã‚‹ãŸã‚ã€é€†é †ã§å‡¦ç†ã—ã€ä¸Šé™ã‚’è¶…ãˆãŸã‚‰åœæ­¢
    for message in reversed(messages):
        # "content" ãŒ None ã®å ´åˆã‚„å­˜åœ¨ã—ãªã„å ´åˆã‚’è€ƒæ…®
        content_to_encode = message.get("content", "") if message.get("content") is not None else ""
        if not isinstance(content_to_encode, str): # contentãŒæ–‡å­—åˆ—ã§ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            content_to_encode = str(content_to_encode)

        tokens_in_message = len(enc.encode(content_to_encode))
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯å¸¸ã«å«ã‚ã‚‹å ´åˆã‚„ã€ç‰¹å®šã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—ã‚’å„ªå…ˆã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚‚æ¤œè¨å¯èƒ½
        # ã“ã“ã§ã¯å˜ç´”ã«ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§åˆ¤æ–­
        if total_tokens + tokens_in_message <= max_tokens:
            trimmed_messages.insert(0, message) # å…ˆé ­ã«è¿½åŠ ã—ã¦å…ƒã®é †åºã‚’ç¶­æŒ
            total_tokens += tokens_in_message
        else:
            # ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™ã‚’è¶…ãˆãŸã®ã§ã€ã“ã‚Œä»¥ä¸Šå¤ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯è¿½åŠ ã—ãªã„
            # é‡è¦ï¼šã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå…ˆé ­ã«ã‚ã‚‹å ´åˆã€ãã‚ŒãŒãƒˆãƒªãƒ ã•ã‚Œã‚‹å¯èƒ½æ€§ã‚’è€ƒæ…®ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
            # ã“ã®å®Ÿè£…ã§ã¯æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰å„ªå…ˆçš„ã«ä¿æŒã™ã‚‹ã€‚
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å¸¸ã«ä¿æŒã—ãŸã„å ´åˆã¯ã€åˆ¥é€”ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¿½åŠ ã™ã‚‹ã€‚
            # ä¾‹: ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æœ€åˆã« `trimmed_messages` ã«è¿½åŠ ã—ã€æ®‹ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³ã§ä»–ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ã™ã‚‹ã€‚
            # ãŸã ã—ã€ä»Šå›ã¯ä¸ãˆã‚‰ã‚ŒãŸã‚·ãƒ³ãƒ—ãƒ«ãªãƒˆãƒªãƒŸãƒ³ã‚°é–¢æ•°ä¾‹[1]ã«å¾“ã†ã€‚
            break
    # st.info(f"å…ƒã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {len(messages)}, ãƒˆãƒªãƒ å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {len(trimmed_messages)}, åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {total_tokens}")
    return trimmed_messages

# Azure AI Search é–¢é€£ã®é–¢æ•°
def get_search_client():
    if AZURE_SEARCH_ENDPOINT and AZURE_SEARCH_KEY:
        return SearchClient(endpoint=AZURE_SEARCH_ENDPOINT,
                            index_name=AZURE_SEARCH_INDEX_NAME,
                            credential=AzureKeyCredential(AZURE_SEARCH_KEY))
    return None

def get_search_index_client():
    if AZURE_SEARCH_ENDPOINT and AZURE_SEARCH_KEY:
        return SearchIndexClient(endpoint=AZURE_SEARCH_ENDPOINT,
                                 credential=AzureKeyCredential(AZURE_SEARCH_KEY))
    return None

def generate_openai_embedding(text):
    if not text or not AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME:
        return None
    try:
        response = client.embeddings.create(
            input=text,
            model=AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME
        )
        return response.data[0].embedding
    except Exception as e:
        st.error(f"Embeddingç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        st.warning("`AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME` ã®è¨­å®šãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return None

def create_search_index_if_not_exists():
    if not AZURE_SEARCH_ENDPOINT or not AZURE_SEARCH_KEY or not AZURE_SEARCH_INDEX_NAME:
        st.warning("Azure AI Search ã®è¨­å®šãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    index_client = get_search_index_client()
    if not index_client:
        return

    try:
        index_client.get_index(AZURE_SEARCH_INDEX_NAME)
        # st.info(f"æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{AZURE_SEARCH_INDEX_NAME}' ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚")
    except Exception:
        st.info(f"æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{AZURE_SEARCH_INDEX_NAME}' ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å®šç¾©
        fields = [
            SimpleField(name="id", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),
            SearchableField(name="question", type=SearchFieldDataType.String, sortable=True, filterable=True),
            SearchableField(name="generated_sql", type=SearchFieldDataType.String, filterable=True),
            SearchableField(name="summary", type=SearchFieldDataType.String),
            SearchField(name="summary_vector", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                        searchable=True, vector_search_dimensions=1536, vector_search_profile_name="my-hnsw-profile"), # text-embedding-ada-002ã€text-embedding-3-smallã¯1536æ¬¡å…ƒ
            SimpleField(name="timestamp", type=SearchFieldDataType.DateTimeOffset, sortable=True, filterable=True)
        ]

        # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ« (HNSWã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨)
        vector_search = VectorSearch(
            profiles=[VectorSearchProfile(name="my-hnsw-profile", algorithm_configuration_name="my-hnsw-config")],
            algorithms=[HnswAlgorithmConfiguration(name="my-hnsw-config")]
        )
        
        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã®è¨­å®š (ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°å‘ä¸Šã«å¯„ä¸)
        # semantic_config = SemanticConfiguration(
        #     name="my-semantic-config",
        #     prioritized_fields=SemanticPrioritizedFields(
        #         title_field=None,
        #         content_fields=[SemanticField(field_name="summary")]
        #     )
        # )
        index = SearchIndex(
            name=AZURE_SEARCH_INDEX_NAME,
            fields=fields,
            vector_search=vector_search,
            # semantic_search = semantic_config # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚’æœ‰åŠ¹ã«ã™ã‚‹å ´åˆ
        )
        try:
            index_client.create_index(index)
            st.success(f"æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{AZURE_SEARCH_INDEX_NAME}' ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
        except Exception as e:
            st.error(f"æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")

# ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å­˜åœ¨ç¢ºèªãƒ»ä½œæˆ
create_search_index_if_not_exists()


def upload_document_to_search(doc_data):
    search_client = get_search_client()
    if not search_client or not doc_data:
        return

    try:
        search_client.upload_documents(documents=[doc_data])
        # st.info(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ID {doc_data['id']} ã‚’æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        st.error(f"æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¸ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")


def search_chat_history(query_text, top_n=3):
    search_client = get_search_client()
    if not search_client or not query_text:
        return []

    try:
        query_embedding = generate_openai_embedding(query_text)
        if not query_embedding:
            st.warning("ã‚¯ã‚¨ãƒªã®Embeddingç”Ÿæˆã«å¤±æ•—ã—ãŸãŸã‚ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¯è¡Œãˆã¾ã›ã‚“ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®ã¿è¡Œã„ã¾ã™ã€‚")
            vector_queries = None
        else:
            vector_queries = [VectorizedQuery(vector=query_embedding, k_nearest_neighbors=top_n, fields="summary_vector")]

        results = search_client.search(
            search_text=query_text, # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ç”¨
            vector_queries=vector_queries, # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ç”¨
            select=["id", "question", "summary", "timestamp"], # å–å¾—ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            top=top_n
        )
        
        return [result for result in results]
    except Exception as e:
        st.error(f"ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return []

# Cosmos DB ä¿å­˜ç”¨é–¢æ•°
from azure.cosmos import CosmosClient, PartitionKey

COSMOS_ENDPOINT = os.getenv("AZURE_COSMOS_ENDPOINT")
COSMOS_KEY = os.getenv("AZURE_COSMOS_KEY")
COSMOS_DB_NAME = os.getenv("AZURE_COSMOS_DATABASE")
COSMOS_CONTAINER_NAME = os.getenv("AZURE_COSMOS_CONTAINER")

def get_cosmos_container():
    try:
        client = CosmosClient(COSMOS_ENDPOINT, COSMOS_KEY)
        db = client.create_database_if_not_exists(id=COSMOS_DB_NAME)
        container = db.create_container_if_not_exists(
            id=COSMOS_CONTAINER_NAME,
            partition_key=PartitionKey(path="/id"),
            offer_throughput=400
        )
        return container
    except Exception as e:
        st.error(f"Cosmos DBæ¥ç¶šã‚¨ãƒ©ãƒ¼ï¼š{e}")
        return None

def save_to_cosmos(new_entry):
    try:
        container = get_cosmos_container()
        if container:
            # new_entryã«idãŒãªã‘ã‚Œã°ä½œæˆ
            if "id" not in new_entry:
                new_entry["id"] = str(uuid.uuid4())
            container.create_item(body=new_entry)
            st.success("Cosmos DBã«ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        st.error(f"Cosmos DBä¿å­˜ã‚¨ãƒ©ãƒ¼ï¼š{e}")


def save_chat_history_to_file(new_entry):
    # ... (æ—¢å­˜ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ä¿å­˜å‡¦ç†) ...
    try:
        if os.path.exists(HISTORY_PATH):
            with open(HISTORY_PATH, "r", encoding="utf-8") as f:
                history = json.load(f)
        else:
            history = []

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«UTCã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’ä»˜ä¸ã—ã¦è¿½åŠ 
        timestamp = datetime.datetime.now(datetime.timezone.utc).isoformat()
        new_entry["timestamp"] = timestamp
        history.append(new_entry)

        with open(HISTORY_PATH, "w", encoding="utf-8") as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"å±¥æ­´ã®ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    # Azure AI Search ã«ã‚‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    if AZURE_SEARCH_ENDPOINT and AZURE_SEARCH_KEY:
        doc_id = str(uuid.uuid4())
        summary_text = new_entry.get("summary_ui") or new_entry.get("summary_chat", "") # summary_ui ã¾ãŸã¯ summary_chat ã‚’ä½¿ã†
        
        search_doc = {
            "id": doc_id,
            "question": new_entry.get("question", ""),
            "generated_sql": new_entry.get("generated_sql", ""),
            "summary": summary_text,
            # new_entryã‹ã‚‰æ­£ã—ã„å½¢å¼ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ä½¿ç”¨
            "timestamp": new_entry.get("timestamp")
        }
        if summary_text:
            embedding = generate_openai_embedding(summary_text)
            if embedding:
                search_doc["summary_vector"] = embedding
        
        upload_document_to_search(search_doc)


# æ–°ã—ã„ä¼šè©±ã‚’å§‹ã‚ã‚‹ãŸã‚ã®é–¢æ•°
def start_new_conversation():
    """
    æ–°ã—ã„ä¼šè©±ã‚’é–‹å§‹ã™ã‚‹ãŸã‚ã«ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã€‚
    é€šå¸¸UIã¨ãƒãƒ£ãƒƒãƒˆUIã®å…¥åŠ›ãƒ»å‡ºåŠ›ã‚’ã™ã¹ã¦ãƒªã‚»ãƒƒãƒˆã—ã€
    AIãŒéå»ã®å¯¾è©±å±¥æ­´ã‚’å¼•ãç¶™ãŒãªã„ã‚ˆã†ã«ã—ã¾ã™ã€‚
    """
    # 1. éå»ã®Q&Aã®æµã‚ŒãŒAIã«å¼•ãç¶™ãŒã‚Œãªã„ã‚ˆã†ã«ã€AIã«æ¸¡ã™ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå±¥æ­´ã‚’ã‚¯ãƒªã‚¢
    st.session_state.chat_history = []
    
    # 2. ç”»é¢ã«è¡¨ç¤ºã™ã‚‹ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’ã‚¯ãƒªã‚¢
    st.session_state.messages = []

    # 3. é€šå¸¸UIã¨ãƒãƒ£ãƒƒãƒˆUIã§ç”Ÿæˆãƒ»ä¿æŒã•ã‚Œã‚‹å„ç¨®æƒ…å ±ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ãŸã‚ã®ã‚­ãƒ¼ãƒªã‚¹ãƒˆ
    keys_to_clear = [
        # é€šå¸¸UIé–¢é€£ã®ã‚­ãƒ¼
        "user_question",            # æœ€åˆã®è³ªå•å…¥åŠ›
        "generated_sql",            # é€šå¸¸UIã§ç”Ÿæˆã•ã‚ŒãŸSQL
        "query_result_ui",          # é€šå¸¸UIã®SQLå®Ÿè¡Œçµæœ(DataFrame)
        "masked_query_result_ui",   # ãƒã‚¹ã‚­ãƒ³ã‚°ã•ã‚ŒãŸå®Ÿè¡Œçµæœ
        "summary_ui",               # é€šå¸¸UIã§ç”Ÿæˆã•ã‚ŒãŸè¦ç´„
        "first_user_question",      # è¦ç´„å¾Œã«ä¿æŒã•ã‚Œã‚‹æœ€åˆã®è³ªå•
        "first_sample_text_csv",    # è¦ç´„å¾Œã«ä¿æŒã•ã‚Œã‚‹æœ€åˆã®çµæœã‚µãƒ³ãƒ—ãƒ«

        # ãƒãƒ£ãƒƒãƒˆUIé–¢é€£ã®ã‚­ãƒ¼
        "generated_sql_chat",       # ãƒãƒ£ãƒƒãƒˆã§ç”Ÿæˆã•ã‚ŒãŸSQL
        "query_result_chat",        # ãƒãƒ£ãƒƒãƒˆã®SQLå®Ÿè¡Œçµæœ(DataFrame)
        "masked_query_result_chat", # ãƒã‚¹ã‚­ãƒ³ã‚°ã•ã‚ŒãŸå®Ÿè¡Œçµæœ
        "summary_chat",             # ãƒãƒ£ãƒƒãƒˆã§ç”Ÿæˆã•ã‚ŒãŸè¦ç´„
        "last_user_input",          # ãƒãƒ£ãƒƒãƒˆã®æœ€å¾Œã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
        "last_ai_response",         # ãƒãƒ£ãƒƒãƒˆã®æœ€å¾Œã®AIå¿œç­”
        "first_sample_text_csv_chat",# ãƒãƒ£ãƒƒãƒˆã®è¦ç´„ã§ä½¿ã‚ã‚ŒãŸçµæœã‚µãƒ³ãƒ—ãƒ«

        # UIã®è¡¨ç¤ºçŠ¶æ…‹ã‚’åˆ¶å¾¡ã™ã‚‹ãƒ•ãƒ©ã‚°
        "show_generated_sql",
        "show_summary_button",
        "show_chat_button",
        "show_generated_sql_chat",
        "show_summary_chat_button"
    ]

    # 4. st.session_stateã«ã‚­ãƒ¼ãŒå­˜åœ¨ã™ã‚Œã°å®‰å…¨ã«å‰Šé™¤
    for key in keys_to_clear:
        if key in st.session_state:
            del st.session_state[key]

    # 5. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚¯ãƒªã‚¢ã•ã‚ŒãŸã“ã¨ã‚’é€šçŸ¥
    st.toast("æ–°ã—ã„ä¼šè©±ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚å…¥åŠ›ã¨å‡ºåŠ›ãŒã‚¯ãƒªã‚¢ã•ã‚Œã¾ã—ãŸã€‚", icon="ğŸ§¹")

    # 6. ç”»é¢ã‚’å³æ™‚æ›´æ–°ã—ã¦å¤‰æ›´ã‚’åæ˜ 
    # st.rerun()ã¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æœ€åˆã‹ã‚‰å†å®Ÿè¡Œã—ã¾ã™ã€‚
    # ã—ã‹ã—ã€Œãƒœã‚¿ãƒ³ã®on_clickãªã©ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°å†…ã€ã§ st.rerun() ã‚’å‘¼ã¶ã¨ã€
    # ãã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®å®Ÿè¡ŒãŒçµ‚ã‚ã£ãŸå¾Œã«ã™ã§ã«StreamlitãŒè‡ªå‹•çš„ã«å†å®Ÿè¡Œã‚’è¡Œã†ãŸã‚ã€æ˜ç¤ºçš„ãªst.rerun()ã¯ç„¡åŠ¹ï¼ˆno-opï¼‰ã«ãªã‚Šã¾ã™ã€‚
    #st.rerun() #Calling st.rerun() within a callback is a no-op.


# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ– ---
if "conn" not in st.session_state:
    st.session_state.conn = None
if "generated_sql" not in st.session_state:
    st.session_state.generated_sql = ""
if "query_result_ui" not in st.session_state:
    st.session_state.query_result_ui = None
if "summary_ui" not in st.session_state:
    st.session_state.summary_ui = ""
if "generated_sql_chat" not in st.session_state:
    st.session_state.generated_sql_chat = ""
if "query_result_chat" not in st.session_state:
    st.session_state.query_result_chat = None
if "summary_chat" not in st.session_state:
    st.session_state.summary_chat = ""
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- ãƒã‚¹ã‚­ãƒ³ã‚°å‡¦ç† ---
def mask_sensitive_data(df, mask_columns):
    df_copy = df.copy()
    for col in mask_columns:
        if col in df_copy.columns:
            df_copy[col] = df_copy[col].astype(str).apply(lambda x: "ï¼Šï¼Šï¼Š" if x else x)
    return df_copy


# 1. DBæ¥ç¶šç®¡ç†
with st.container():
    op_col1, op_col2, op_col3 = st.columns([1, 1, 6])
    with op_col1:
        st.markdown("### DBæ“ä½œ")
    with op_col2:
        connect_clicked = st.button("DBæ¥ç¶šï¼")
    with op_col3:
        disconnect_clicked = st.button("DBåˆ‡æ–­")

if connect_clicked:
    try:
        st.session_state.conn = pyodbc.connect(connection_string)
        st.success("æ¥ç¶šæˆåŠŸï¼catalog_mstãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
        query = "SELECT TOP 2 * FROM catalog_mst;"
        df = pd.read_sql(query, st.session_state.conn)
        st.dataframe(df)
    except Exception as e:
        st.error(f"æ¥ç¶šã‚¨ãƒ©ãƒ¼ï¼š{e}")
elif disconnect_clicked:
    try:
        if st.session_state.conn:
            st.session_state.conn.close()
            st.session_state.conn = None
            st.success("DBæ¥ç¶šã‚’åˆ‡æ–­ã—ã¾ã—ãŸã€‚")
        else:
            st.info("ã™ã§ã«åˆ‡æ–­ã•ã‚Œã¦ã„ã¾ã™ã€‚")
    except Exception as e:
        st.error(f"åˆ‡æ–­æ™‚ã‚¨ãƒ©ãƒ¼ï¼š{e}")

# 2. è‡ªç„¶è¨€èªã§è³ªå•å…¥åŠ›
st.markdown("### è³ªå•ã‚’å…¥åŠ›")
#st.text_inputã®åˆæœŸå€¤ï¼ˆç¬¬2å¼•æ•°ï¼‰ã‚’st.session_state.get("user_question", "")ã«ã—ã¦ã€ã‚¯ãƒªã‚¢æ™‚ã«st.session_state["user_question"] = ""ã¨ã™ã‚‹ã€‚
user_question = st.text_input("ä¾‹ï¼šå–¶æ¥­éƒ¨ã«æ‰€å±ã—ã¦ã„ã‚‹å¾“æ¥­å“¡ã®ä¸€è¦§ã‚’è¦‹ã›ã¦ã€‚",value=st.session_state.get("user_question", ""), key="user_question")

# 3. ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ¼ãƒã‚’Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
schema_text = ""
#ãƒ•ã‚©ãƒ«ãƒ€åç§°
prompts_dir = "src/prompts_nltosql"
if os.path.isdir(prompts_dir):
    #ã“ã®os.listdir ã¯ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã®ä¸­èº«ã‚’æ¢ç´¢ã—ãªã„
    for filename in os.listdir(prompts_dir):
        if filename.endswith(".md"):
            with open(os.path.join(prompts_dir, filename), "r", encoding="utf-8") as f:
                schema_text += f"\n---\n# {filename}\n" + f.read() + "\n"

# 4. é€šå¸¸ã®UIå´ã§è‡ªç„¶è¨€èª â†’ SQL å¤‰æ›ï¼ˆAzure OpenAIåˆ©ç”¨ï¼‰
if st.button("SQLä½œæˆ") and user_question:
    with st.spinner("SQLã«å¤‰æ›ä¸­..."):
        try:
            #ãƒ•ã‚©ãƒ«ãƒ€åç§°
            nltosql_prompt_path = "src/prompts_nltosql/nltosql/01_natural_language_to_sql.md"
            if not os.path.exists(nltosql_prompt_path):
                st.warning(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {nltosql_prompt_path}")
            else:
                with open(nltosql_prompt_path, "r", encoding="utf-8") as f:
                    nltosql_prompt = f.read()

                prompt = (
                    "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆDBMSï¼‰ã®ç¨®é¡ã¯ã€ŒSQL Serverã€ã§ã™ã€‚"
                    "SQLã«ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¯ä¸è¦ã§ã™ã€‚ã€Œ```sqlã€ã‚„ã€Œ```ã€ã¯ä¸è¦ã§ã™ã€‚SQLã«è§£èª¬ã¯ä¸è¦ã§ã™ã€‚å®Ÿè¡Œå¯èƒ½ãªSQLã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"
                    "SQLã«ã¯LIMITå¥ã‚’ä½¿ç”¨ã—ã¾ã›ã‚“"
                    "æ¬¡ã®ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©ã«åŸºã¥ãã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾å¿œã™ã‚‹SQLã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
                )

                messages_for_sql=[
                    {"role": "system", "content": "ã‚ãªãŸã¯SQLç”Ÿæˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                    {"role": "system", "content": prompt},
                    {"role": "system", "content": f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©ã‚„ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ : {schema_text}\n\n"},
                    {"role": "system", "content": nltosql_prompt},
                    {"role": "user", "content": f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {user_question}"}
                ]
                # é€ä¿¡å‰ã«ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’åˆ¶é™
                trimmed_messages_for_sql = trim_messages_to_token_limit(messages_for_sql, max_tokens=10000, model_name="gpt-4o-mini")

                response = client.chat.completions.create(
                    model=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
                    messages=trimmed_messages_for_sql,
                    max_tokens=500 # å—ä¿¡ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®ä¸Šé™
                )

                st.session_state.generated_sql = response.choices[0].message.content.strip()
                st.success("SQLç”ŸæˆæˆåŠŸï¼")
        except Exception as e:
            st.error(f"SQLç”Ÿæˆã‚¨ãƒ©ãƒ¼ï¼š{e}")

# 5. é€šå¸¸ã®UIå´ã§ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¨SQLè¡¨ç¤º
if st.session_state.generated_sql:
    st.markdown("### ç”Ÿæˆã•ã‚ŒãŸSQLï¼š")
    st.code(st.session_state.generated_sql, language="sql")

    #upper_sql = st.session_state.generated_sql.strip().upper()
    #if not upper_sql.startswith("SELECT"):
    #    st.warning("SELECTæ–‡ä»¥å¤–ã®è¨˜è¿°ã‚„ã‚³ãƒ¡ãƒ³ãƒˆãŒå«ã¾ã‚Œã¦ã„ã‚‹ãŸã‚ã€SQLã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã€‚")
    #else:

    # 6. SQLå®Ÿè¡Œ(SELECTæ–‡ã®ã¿)
    if st.button("SQLå®Ÿè¡Œ"):
        try:
            df = pd.read_sql(st.session_state.generated_sql, st.session_state.conn)
            st.session_state.query_result_ui = df

            #å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’å–å¾—ã—ã¦ãƒã‚¹ã‚­ãƒ³ã‚°é©ç”¨
            match = re.search(r'FROM\s+([a-zA-Z0-9_]+)', st.session_state.generated_sql, re.IGNORECASE)
            table_name = match.group(1) if match else ""
            mask_columns = get_mask_columns(table_name)
            masked_df = mask_sensitive_data(df, mask_columns)
            st.session_state.masked_query_result_ui = masked_df
            #st.dataframe(masked_df)
            st.success("SQLã‚’å®Ÿè¡Œã—ã¾ã—ãŸ")
        except Exception as e:
            st.error(f"SQLå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ï¼š{e}")

# SQLå®Ÿè¡Œçµæœã‚’å¸¸ã«è¡¨ç¤º(é€šå¸¸ã®UIå´)
if st.session_state.query_result_ui is not None:
    st.markdown("### SQLå®Ÿè¡Œçµæœ")
    st.dataframe(st.session_state.masked_query_result_ui)

# 7. è¦ç´„ã¨çµæœã®èª¬æ˜(é€šå¸¸ã®UIå´)
if st.session_state.query_result_ui is not None:
    if st.button("è¦ç´„ã¨çµæœã®èª¬æ˜") and st.session_state.query_result_ui is not None:
        with st.spinner("è¦ç´„ã¨çµæœã®èª¬æ˜ã‚’ä½œæˆä¸­..."):
            try:
                df_sample = st.session_state.masked_query_result_ui.head(8)
                sample_text_csv = df_sample.to_csv(index=False)

                #ãƒ•ã‚©ãƒ«ãƒ€åç§°
                summarize_prompt_path = "src/prompts_summary/summarilze_results_prompt.md"
                with open(summarize_prompt_path, "r", encoding="utf-8") as f:
                    summarize_prompt = f.read()
                messages_for_summary=[
                    {"role": "system", "content": "ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿è¦ç´„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                    {"role": "user", "content": f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {user_question}"},
                    {"role": "user", "content": f"å®Ÿè¡Œã—ãŸSQL:\n{st.session_state.generated_sql}"},
                    {"role": "user", "content": f"CSVãƒ‡ãƒ¼ã‚¿:\n{sample_text_csv}"},
                    {"role": "user", "content": summarize_prompt}
                ]
                # é€ä¿¡å‰ã«ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’åˆ¶é™
                trimmed_messages_for_summary = trim_messages_to_token_limit(messages_for_summary, max_tokens=10000, model_name="gpt-4o-mini")

                summary_response = client.chat.completions.create(
                    model=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
                    messages=trimmed_messages_for_summary,
                    max_tokens=50
                )
                ## AIã‹ã‚‰ã®è¿”ç­”ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³(session_state.summary_ui)ã«ä¿å­˜
                st.session_state.summary_ui = summary_response.choices[0].message.content.strip()
                st.session_state.first_user_question = user_question  # ã“ã®user_questionã‚‚SQLä½œæˆæ™‚ã®ã‚‚ã®
                st.session_state.first_sample_text_csv = sample_text_csv
                # ç¢ºèªè¡¨ç¤º
                #st.write(st.session_state.first_sample_text_csv)
                #st.markdown("### â– è¦ç´„")
                # AIã‹ã‚‰ã®è¿”ç­”ã‚’è¡¨ç¤º
                #st.write(st.session_state.summary_ui)

                # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ (é€šå¸¸ã®UIå´)
                chat_entry = {
                    "question": user_question,  # SQLä½œæˆæ™‚ã®è³ªå•
                    "generated_sql": st.session_state.generated_sql,
                    "result_sample": json.loads(st.session_state.query_result_ui.head(8).to_json(orient="records")),
                    #"result_sample": json.loads(df_sample.to_json(orient="records")),
                    "summary_ui": st.session_state.summary_ui
                }
                st.session_state.chat_history.append(chat_entry)
                save_chat_history_to_file(chat_entry) # å†…éƒ¨ã§AISearchã«ä¿å­˜
                save_to_cosmos(chat_entry)# å†…éƒ¨ã§CosmosDBã«ä¿å­˜
            except Exception as e:
                st.error(f"è¦ç´„ã¨çµæœã®èª¬æ˜ã®ç”Ÿæˆã‚¨ãƒ©ãƒ¼ï¼š{e}")

# è¦ç´„è¡¨ç¤ºã‚’å¸¸ã«ç¶­æŒ(é€šå¸¸ã®UIå´)
if st.session_state.summary_ui:
    st.markdown("### è¦ç´„")
    #AIã‹ã‚‰ã®è¿”ç­”ã‚’è¡¨ç¤º
    st.write(st.session_state.summary_ui)

# 8. ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ç¢ºèª(å·¦ãƒ‘ãƒãƒ«)
with st.sidebar:
    st.markdown("### ä¼šè©±æ“ä½œ")
    # on_clickã«è¨­å®šã—ãŸé–¢æ•°å†…ã§ st.rerun() ã‚’å‘¼ã¶ãŸã‚ã€ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸå¾Œã® st.rerun() ã¯ä¸è¦ã§ã™ã€‚
    st.button('æ–°ã—ã„ä¼šè©±ã‚’å§‹ã‚ã‚‹', on_click=start_new_conversation, key="new_conversation_btn")

    st.markdown("---") # åŒºåˆ‡ã‚Šç·š

    st.markdown("### è¦ç´„ã¨çµæœèª¬æ˜ã€é€ä¿¡ã®å±¥æ­´")
    show_panel = st.checkbox("è¡¨ç¤ºã™ã‚‹", value=False, key="toggle_history")
    if show_panel:
        if os.path.exists(HISTORY_PATH):
            with open(HISTORY_PATH, "r", encoding="utf-8") as f:
                try:
                    history = json.load(f) # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®å±¥æ­´
                except json.JSONDecodeError:
                    history = []
                for item in reversed(history[-5:]): #è¡¨ç¤ºã™ã‚‹ã®ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®å±¥æ­´ï¼ˆæœ€å¤§5ä»¶ï¼‰
                    st.markdown(f"**æ—¥æ™‚ï¼š** {item.get('timestamp', '')}")
                    st.markdown(f"**è³ªå•ï¼š** {item.get('question', '')}")
                    st.markdown(f"**ç”ŸæˆSQLï¼š** `{item.get('generated_sql', '')}`")
                    if "summary_ui" in item:
                        st.markdown(f"**è¦ç´„ï¼š** {item['summary_ui']}")
                    if "summary_chat" in item:
                        st.markdown(f"**è¦ç´„ï¼ˆãƒãƒ£ãƒƒãƒˆï¼‰:** {item['summary_chat']}")
                st.markdown("---")
        else:
            st.info("å±¥æ­´ãŒã¾ã å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")

# å±¥æ­´ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ›ã™ã‚‹é–¢æ•°
def build_chat_context(history, limit=2):  # æœ€æ–°ã®2ä»¶or3ä»¶ã¾ã§ã«çµã‚‹ï¼ˆTokenç¯€ç´„ï¼‰
    messages = []
    for h in history[-limit:]:  # æœ€æ–°ã®2ä»¶or3ä»¶ã¾ã§ã«çµã‚‹ï¼ˆTokenç¯€ç´„ï¼‰
        messages.append({"role": "user", "content": f"è³ªå•: {h.get('question', '')}"})
        if h.get('generated_sql'): # SQLãŒã‚ã‚‹å ´åˆã®ã¿è¿½åŠ 
            messages.append({"role": "user", "content": f"ç”ŸæˆSQL:\n{h.get('generated_sql', '')}"})
        if "summary_ui" in h:
            messages.append({"role": "assistant", "content": f"è¦ç´„: {h['summary_ui']}"})
        elif "summary_chat" in h:
            messages.append({"role": "assistant", "content": f"è¦ç´„ï¼ˆãƒãƒ£ãƒƒãƒˆï¼‰: {h['summary_chat']}"})
    return messages

# 9. ãƒãƒ£ãƒƒãƒˆã®UIå´ã€å¯¾è©±å‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ã‚¤ã‚¹ï¼ˆè¦ç´„å¾Œã«å…¥åŠ›å—ä»˜ï¼‰
if st.session_state.summary_ui:
    st.markdown("### ãƒãƒ£ãƒƒãƒˆå½¢å¼ã§è³ªå•")
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ç®¡ç†
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤ºï¼ˆã¾ãšå±¥æ­´ã ã‘ä¸Šã«è¡¨ç¤ºï¼‰
    for msg in st.session_state.messages:
        if msg["role"] == "user":
            st.markdown(f"**ã‚ãªãŸ:** {msg['content']}")
        else:
            st.markdown(f"**AI:** {msg['content']}")

    # --- ã“ã“ã‹ã‚‰ä¸‹ã«å…¥åŠ›æ¬„ã¨ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º ---
    st.divider()  # ä»•åˆ‡ã‚Šç·šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰


    with st.form(key="chat_form", clear_on_submit=True):
        user_input = st.text_area("ç¶šã‘ã¦è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", height=100)
        with st.container():
            op_col1, op_col2, op_col3 = st.columns([1, 5, 8])  # å³å´ã«å¤§ããªç©ºç™½ã‚«ãƒ©ãƒ 
            with op_col1:
                submitted = st.form_submit_button("é€ä¿¡")
            with op_col2:
                nltosqlmake_clicked = st.form_submit_button("SQLä½œæˆ (ãƒãƒ£ãƒƒãƒˆç”¨)")
            # op_col3ã¯ç©ºç™½ã‚¹ãƒšãƒ¼ã‚¹ç”¨


        if submitted and user_input:
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
            st.session_state.messages.append({"role": "user", "content": user_input})
            
            # ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¡¨ç¤º
            with st.spinner('AIãŒè€ƒãˆä¸­... (æ¤œç´¢ã¨å¿œç­”)'):
                try:
                    # Azure AI Searchã‹ã‚‰é–¢é€£æƒ…å ±ã‚’å–å¾—
                    retrieved_search_context_str = ""
                    if AZURE_SEARCH_ENDPOINT and AZURE_SEARCH_KEY:
                        search_results = search_chat_history(user_input, top_n=2) # ä¸Šä½2ä»¶ã‚’å–å¾—
                        if search_results:
                            retrieved_search_context_str += "ã€é–¢é€£ã™ã‚‹éå»ã®ã‚„ã‚Šå–ã‚Šã€‘:\n"
                            for result in search_results:
                                retrieved_search_context_str += f"- è³ªå•: {result.get('question', 'N/A')}\n  è¦ç´„: {result.get('summary', 'N/A')}\n"
                            retrieved_search_context_str += "\n---\n"
                    # å±¥æ­´ã‹ã‚‰éå»ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰ï¼ˆæœ€æ–°2ä»¶ï¼‰
                    #history_messages = build_chat_context(st.session_state.chat_history, limit=2)   # æœ€æ–°ã®2ä»¶or3ä»¶ã¾ã§ã«çµã‚‹ï¼ˆTokenç¯€ç´„ï¼‰
                    #st.write(history_messages )#ç¢ºèªçµæœ
                    # å±¥æ­´ã‹ã‚‰éå»ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰ï¼ˆæœ€æ–°2ä»¶ï¼‰ + AI Searchçµæœ + ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
                    history_messages_for_chat = build_chat_context(st.session_state.chat_history, limit=2)
                    constructed_messages_for_chat = []
                    # æ—¢å­˜ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ï¼ˆç”»é¢è¡¨ç¤ºç”¨ï¼‰ã‚‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã‚ã‚‹å ´åˆ
                    # constructed_messages_for_chat.extend(st.session_state.messages[-3:]) # æœ€æ–°ã®ãƒãƒ£ãƒƒãƒˆæ•°ä»¶

                    # éå»ã®DBæ“ä½œå±¥æ­´
                    constructed_messages_for_chat.extend(history_messages_for_chat)
                    
                    # Azure AI Search ã‹ã‚‰å–å¾—ã—ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
                    if retrieved_search_context_str:
                         constructed_messages_for_chat.append({"role": "system", "content": retrieved_search_context_str})
                    

                    # ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’è¿½åŠ 
                    #current_input = {"role": "user", "content": user_input}
                    #messages = history_messages + [current_input]
                    constructed_messages_for_chat.append({"role": "user", "content": user_input})

                    # é€ä¿¡å‰ã«ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’åˆ¶é™
                    trimmed_messages_for_chat_response = trim_messages_to_token_limit(constructed_messages_for_chat, max_tokens=10000, model_name="gpt-4o-mini")

                    #response = client.chat.completions.create(
                    #    model=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
                    #    messages=messages
                    #
                    response = client.chat.completions.create(
                        model=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"), # GPT-4o miniã‚’æŒ‡å®š
                        messages=trimmed_messages_for_chat_response,
                        max_tokens=500
                    )

                    ai_response = response.choices[0].message.content.strip()
                    st.session_state.messages.append({"role": "assistant", "content": ai_response})
                    # æœ€æ–°ã®è³ªå•ã‚‚å±¥æ­´ã¨ã—ã¦ä¿æŒï¼ˆã‚ã¨ã§è¦ç´„ã‚„SQLç”Ÿæˆæ™‚ã«å†åˆ©ç”¨å¯èƒ½ï¼‰
                    st.session_state.last_user_input = user_input
                    st.session_state.last_ai_response = ai_response
                    # ãƒãƒ£ãƒƒãƒˆã®ã‚„ã‚Šå–ã‚Šã‚‚æ¤œç´¢å¯¾è±¡ã¨ã—ã¦ä¿å­˜ã™ã‚‹å ´åˆ 
                    # ã“ã®æ™‚ç‚¹ã§ã¯SQLã‚„DBã‹ã‚‰ã®çµæœãŒãªã„ãŸã‚ã€è³ªå•ã¨AIã®ä¸€èˆ¬çš„ãªå¿œç­”ã‚’ä¿å­˜ã™ã‚‹å½¢ã«ãªã‚‹
                    # ã‚‚ã—ã€ã“ã®AIå¿œç­”ãŒã€Œè¦ç´„ã€ã«é¡ã™ã‚‹ã‚‚ã®ã§ã‚ã‚Œã°ã€ä¿å­˜ã™ã‚‹ä¾¡å€¤ãŒã‚ã‚‹

                    current_chat_entry_for_search = {
                         "question": user_input,
                        "summary_chat": ai_response, # "summary_chat" ã¨ã—ã¦AIã®å¿œç­”ã‚’ä¿å­˜
                        # ãƒãƒ£ãƒƒãƒˆã®ã‚„ã‚Šå–ã‚Šä¿å­˜æ™‚ã‚‚UTCã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»˜ãã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ä½¿ç”¨
                        "timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat()
                    }
                    save_chat_history_to_file(current_chat_entry_for_search)  # AISearchã«ä¿å­˜
                    save_to_cosmos(current_chat_entry_for_search) # CosmosDBã«ä¿å­˜
                    # ã™ããƒªãƒ­ãƒ¼ãƒ‰ã—ã¦æ–°ã—ã„ä¼šè©±ã‚’è¡¨ç¤º
                    st.rerun()
                except Exception as e:
                    st.error(f"ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚¨ãƒ©ãƒ¼ï¼ˆæ¤œç´¢å«ã‚€ï¼‰ï¼š{e}")

        if nltosqlmake_clicked and user_input:
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ (ãƒãƒ£ãƒƒãƒˆã®ä¸Šéƒ¨ã®å±¥æ­´ã¸)
            st.session_state.messages.append({"role": "user", "content": user_input})

            with st.spinner("ãƒãƒ£ãƒƒãƒˆã®è³ªå•ã‚’SQLã«å¤‰æ›ä¸­..."):
                try:
                    #ãƒ•ã‚©ãƒ«ãƒ€åç§°
                    nltosql_prompt_path = "src/prompts_nltosql/nltosql/01_natural_language_to_sql.md"
                    if not os.path.exists(nltosql_prompt_path):
                        st.warning(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {nltosql_prompt_path}")
                    else:
                        with open(nltosql_prompt_path, "r", encoding="utf-8") as f:
                            nltosql_prompt_content_chat = f.read()

                        system_prompt_sql_chat = (
                            "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆDBMSï¼‰ã®ç¨®é¡ã¯ã€ŒSQL Serverã€ã§ã™ã€‚"
                            "SQLã«ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¯ä¸è¦ã§ã™ã€‚ã€Œ```sqlã€ã‚„ã€Œ```ã€ã¯ä¸è¦ã§ã™ã€‚SQLã«è§£èª¬ã¯ä¸è¦ã§ã™ã€‚å®Ÿè¡Œå¯èƒ½ãªSQLã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"
                            "SQLã«ã¯LIMITå¥ã‚’ä½¿ç”¨ã—ã¾ã›ã‚“"
                            "æ¬¡ã®ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©ã«åŸºã¥ãã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾å¿œã™ã‚‹SQLã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
                        )

                        messages_for_chat_sql = [
                            {"role": "system", "content": "ã‚ãªãŸã¯SQLç”Ÿæˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                            {"role": "system", "content": system_prompt_sql_chat},
                            {"role": "system", "content": f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©ã‚„ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ : {schema_text}\n\n"},
                            {"role": "system", "content": nltosql_prompt_content_chat},
                            {"role": "user", "content": f"æœ€æ–°ã®è³ªå•: {user_input}"},
                        ]
                        
                        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‹ã‚‰ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚‚è¿½åŠ ã™ã‚‹å ´åˆ (ä¾‹: ç›´å‰ã®ä¼šè©±)
                        # chat_context_messages = st.session_state.messages[-3:] # æœ€æ–°3ä»¶ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã€AIå¿œç­”ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ï¼‰
                        # messages_for_chat_sql = chat_context_messages + messages_for_chat_sql

                        # é€ä¿¡å‰ã«ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’åˆ¶é™
                        trimmed_messages_for_chat_sql = trim_messages_to_token_limit(messages_for_chat_sql, max_tokens=10000, model_name="gpt-4o-mini")

                        response = client.chat.completions.create(
                            model=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"), # GPT-4o miniã‚’æŒ‡å®š
                            messages=trimmed_messages_for_chat_sql,
                            max_tokens=500
                        )

                        # AIã‹ã‚‰ã®è¿”ç­”ã‚’å±¥æ­´ã«è¿½åŠ 
                        ai_sql = response.choices[0].message.content.strip()
                        st.session_state.generated_sql_chat = ai_sql
                        #st.write(st.session_state.generated_sql_chat )
                        st.success("SQLç”ŸæˆæˆåŠŸï¼")
                        #ãƒãƒ£ãƒƒãƒˆã®ä¸Šéƒ¨ã®å±¥æ­´ã¸
                        st.session_state.messages.append({"role": "assistant", "content": f"(ç”ŸæˆSQL) {ai_sql}"})
                        st.session_state.last_user_input = user_input
                        #st.write(st.session_state.last_user_input )
                        # ã™ããƒªãƒ­ãƒ¼ãƒ‰ã—ã¦æ–°ã—ã„ä¼šè©±ã‚’è¡¨ç¤º
                        st.rerun()
                except Exception as e:
                    st.error(f"SQLç”Ÿæˆã‚¨ãƒ©ãƒ¼ï¼š{e}")

# 10. ãƒãƒ£ãƒƒãƒˆã®UIå´ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¨SQLè¡¨ç¤º
if st.session_state.generated_sql_chat:
    st.markdown("### ç”Ÿæˆã•ã‚ŒãŸSQLï¼š")
    st.code(st.session_state.generated_sql_chat, language="sql")

    #upper_sql = st.session_state.generated_sql_chat.strip().upper()
    #if not upper_sql.startswith("SELECT"):
    #    st.warning("SELECTæ–‡ä»¥å¤–ã®è¨˜è¿°ã‚„ã‚³ãƒ¡ãƒ³ãƒˆãŒå«ã¾ã‚Œã¦ã„ã‚‹ãŸã‚ã€SQLã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã€‚")
    #else:

    # 6. SQLå®Ÿè¡Œ(SELECTæ–‡ã®ã¿)
    #if st.button("SQLå®Ÿè¡Œ (ãƒãƒ£ãƒƒãƒˆç”¨)"):
    col1, col2, col3 = st.columns([3, 5, 8])
    with col1:
        exec_clicked = st.button("SQLå®Ÿè¡Œ (ãƒãƒ£ãƒƒãƒˆç”¨)")
    with col2:
        cancel_clicked = st.button("æˆ»ã‚‹ (SQLå®Ÿè¡Œã—ãªã„)")
    # op_col3ã¯ç©ºç™½ã‚¹ãƒšãƒ¼ã‚¹ç”¨


    if exec_clicked:
        try:
            df = pd.read_sql(st.session_state.generated_sql_chat, st.session_state.conn)
            st.session_state.query_result_chat = df

            #å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’å–å¾—ã—ã¦ãƒã‚¹ã‚­ãƒ³ã‚°é©ç”¨
            match = re.search(r'FROM\s+([a-zA-Z0-9_]+)', st.session_state.generated_sql_chat, re.IGNORECASE)
            table_name = match.group(1) if match else ""
            mask_columns = get_mask_columns(table_name)

            masked_df = mask_sensitive_data(df, mask_columns)
            st.session_state.masked_query_result_chat = masked_df
            #st.dataframe(masked_df)
            
            # è¡¨ç¤ºç”¨ã«CSVå½¢å¼ã«å¤‰æ›ï¼ˆå…ˆé ­5è¡Œãªã©ã«åˆ¶é™å¯ï¼‰
            df_preview = st.session_state.masked_query_result_chat.head(5)  # å¿…è¦ã«å¿œã˜ã¦åˆ¶é™
            result_text = df_preview.to_markdown(index=False)  # Markdownã§è¡¨å½¢å¼
            #ãƒãƒ£ãƒƒãƒˆã®ä¸Šéƒ¨ã®å±¥æ­´ã¸
            st.session_state.messages.append({"role": "assistant", "content": f"SQLå®Ÿè¡Œçµæœï¼ˆä¸Šä½5ä»¶ï¼‰:\n```\n{result_text}\n```"})
            # é€šå¸¸ã®UIã«ã‚‚è¡¨ç¤ºï¼ˆä»»æ„ï¼‰
            #st.markdown("### SQLå®Ÿè¡Œçµæœ(ãƒãƒ£ãƒƒãƒˆ)")
            #st.dataframe(st.session_state.masked_query_result_chat)
            st.success("SQLã‚’å®Ÿè¡Œã—ã¾ã—ãŸ")
            st.rerun()  # ä¸Šéƒ¨ã®å±¥æ­´ã«å³åº§ã«åæ˜ ã•ã›ã‚‹ãŸã‚ã«å¿…è¦
        except Exception as e:
            st.error(f"SQLå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ï¼š{e}")
    
    # SQLå®Ÿè¡Œçµæœã‚’å¸¸ã«è¡¨ç¤º(é€šå¸¸ã®UIå´)
    if st.session_state.query_result_chat is not None:
        st.markdown("### SQLå®Ÿè¡Œçµæœ(ãƒãƒ£ãƒƒãƒˆ)")
        st.dataframe(st.session_state.masked_query_result_chat)

    if cancel_clicked:
        st.session_state.generated_sql_chat = ""
        st.session_state.query_result_chat = None
        st.session_state.masked_query_result_chat = None
        st.session_state.summary_chat = ""
        st.session_state.show_generated_sql_chat = False
        st.session_state.show_summary_chat_button = False
        st.rerun()



# 11. è¦ç´„ã¨çµæœã®èª¬æ˜(ãƒãƒ£ãƒƒãƒˆã®UIå´)
if st.session_state.query_result_chat is not None:
    if st.button("è¦ç´„ã¨çµæœã®èª¬æ˜ (ãƒãƒ£ãƒƒãƒˆç”¨)"):
        with st.spinner("ãƒãƒ£ãƒƒãƒˆã®è¦ç´„ã¨çµæœã®èª¬æ˜ã‚’ä½œæˆä¸­..."):
            try:
                df_sample_chat = st.session_state.masked_query_result_chat.head(8)
                sample_text_csv_chat = df_sample_chat.to_csv(index=False)

                #ãƒ•ã‚©ãƒ«ãƒ€åç§°
                summarize_prompt_path = "src/prompts_summary/summarilze_results_prompt.md"
                with open(summarize_prompt_path, "r", encoding="utf-8") as f:
                    summarize_prompt_content_chat = f.read()

                messages_for_chat_summary=[
                    {"role": "system", "content": "ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿è¦ç´„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                    {"role": "user", "content": f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {st.session_state.last_user_input}"}, # ç›´å‰ã®ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
                    #{"role": "user", "content": f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {user_input}"},
                    {"role": "user", "content": f"å®Ÿè¡Œã—ãŸSQL:\n{st.session_state.generated_sql_chat}"},
                    {"role": "user", "content": f"CSVãƒ‡ãƒ¼ã‚¿:\n{sample_text_csv_chat}"},
                    {"role": "user", "content": summarize_prompt_content_chat}
                ]

                # é€ä¿¡å‰ã«ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’åˆ¶é™
                trimmed_messages_for_chat_summary = trim_messages_to_token_limit(messages_for_chat_summary, max_tokens=10000, model_name="gpt-4o-mini")

                summary_response_chat = client.chat.completions.create(
                    model=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"), # GPT-4o miniã‚’æŒ‡å®š
                    messages=trimmed_messages_for_chat_summary,
                    max_tokens=500
                )

                ## AIã‹ã‚‰ã®è¿”ç­”ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³(session_state.summary_chat)ã«ä¿å­˜
                summary_text  = summary_response_chat.choices[0].message.content.strip()
                st.session_state.summary_chat = summary_text
                st.session_state.first_user_question = user_question
                st.session_state.first_sample_text_csv_chat = sample_text_csv_chat

                # ãƒãƒ£ãƒƒãƒˆã«è¦ç´„ã‚’ä¸Šéƒ¨ã®å±¥æ­´ã«å³æ™‚åæ˜ 
                st.session_state.messages.append({"role": "assistant", "content": f"(ç”Ÿæˆã—ãŸSQL) {st.session_state.generated_sql_chat}\n\nç”Ÿæˆã—ãŸSQLã®è¦ç´„: \n\n{st.session_state.summary_chat}"})

                # ç¢ºèªè¡¨ç¤º
                #st.write(st.session_state.first_sample_text_csv_chat)
                # AIã‹ã‚‰ã®è¿”ç­”ã‚’è¡¨ç¤º
                #st.write(st.session_state.summary_chat)

                # ãƒ•ã‚¡ã‚¤ãƒ«ã¨CosmosDBã¸ã®ä¿å­˜ç”¨)
                chat_entry_summary = {
                    "question": st.session_state.last_user_input,
                    "generated_sql": st.session_state.generated_sql_chat,
                    "result_sample": json.loads(df_sample_chat.to_json(orient="records")),
                    "summary_chat": st.session_state.summary_chat, # summary_chat ã¨ã—ã¦ä¿å­˜
                    "timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat()
                }
                st.session_state.chat_history.append(chat_entry_summary)
                save_chat_history_to_file(chat_entry_summary) # å†…éƒ¨ã§AISearchã«ä¿å­˜
                save_to_cosmos(chat_entry_summary) # CosmosDBã«ä¿å­˜

                st.session_state.generated_sql_chat = ""
                st.session_state.query_result_chat = None
                st.session_state.masked_query_result_chat = None
                st.session_state.summary_chat = ""
                st.session_state.show_generated_sql_chat = False
                st.session_state.show_summary_chat_button = False

                st.success("è¦ç´„ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
                st.rerun()  # ä¸Šéƒ¨ã®å±¥æ­´ã«å³æ™‚åæ˜ ã€‚st.rerun() ã‚’å‘¼ã¶ã¨ã€ãã‚Œä»¥é™ã®ã‚³ãƒ¼ãƒ‰ã¯ä¸€åˆ‡å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“ã€‚
            except Exception as e:
                st.error(f"è¦ç´„ã¨çµæœã®èª¬æ˜ã®ç”Ÿæˆã‚¨ãƒ©ãƒ¼ï¼š{e}")
